{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 初始化\n",
    "## 1.1 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import random\n",
    "from datetime import date, datetime\n",
    "import datetime\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 读入文件\n",
    "\n",
    "存放在 list 中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_fqdn = []\n",
    "with open('fqdn.csv', 'r', encoding='utf-8') as f:\n",
    "    f_fqdn = csv.reader(f)\n",
    "    headers = next(f_fqdn) # 跳过第一行\n",
    "    for row in f_fqdn:\n",
    "        l_fqdn.append(row)\n",
    "\n",
    "l_ip = {}\n",
    "with open('ip.csv', 'r', encoding='utf-8') as f:\n",
    "    f_ip = csv.reader(f)\n",
    "    headers = next(f_ip)\n",
    "    for row in f_ip:\n",
    "        l_ip[row[0]] = row[1:]\n",
    "\n",
    "# l_ipv6 = []\n",
    "with open('ipv6.csv', 'r', encoding='utf-8') as f:\n",
    "    f_ipv6 = csv.reader(f)\n",
    "    headers = next(f_ipv6)\n",
    "    for row in f_ipv6:\n",
    "        # l_ipv6[row[0]] = row[1:]\n",
    "        l_ip[row[0]] = row[1:]\n",
    "\n",
    "l_access = []\n",
    "with open('access.csv', 'r', encoding='utf-8') as f:\n",
    "    f_access = csv.reader(f)\n",
    "    headers = next(f_access)\n",
    "    for row in f_access:\n",
    "        l_access.append(row)\n",
    "\n",
    "l_flint = []\n",
    "with open('flint.csv', 'r', encoding='utf-8') as f:\n",
    "    f_flint = csv.reader(f)\n",
    "    headers = next(f_flint)\n",
    "    for row in f_flint:\n",
    "        l_flint.append(row)\n",
    "\n",
    "l_label = []\n",
    "with open('label.csv', 'r', encoding='utf-8') as f:\n",
    "    f_label = csv.reader(f)\n",
    "    headers = next(f_label)\n",
    "    for row in f_label:\n",
    "        l_label.append(row)\n",
    "\n",
    "l_whois = []\n",
    "with open('whois.json', 'r', encoding='utf-8') as f:\n",
    "    f_whois = json.load(f)\n",
    "    for row in f_whois:\n",
    "        l_whois.append(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 特征提取\n",
    "\n",
    "## 2.1 access\n",
    "\n",
    "将 access 与 ip 一并处理。选取的特征有\n",
    "\n",
    "- 访问次数\n",
    "- 连续访问次数\n",
    "- 访问 IP 数\n",
    "- 访问国家数\n",
    "- 访问城市数\n",
    "- 访问 ISP 数\n",
    "- 按小时统计访问次数\n",
    "- 按日期统计访问次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "tot_fqdn = 20512 # fqdn 编号总数\n",
    "'''\n",
    "由于需要直接访问下标，所以需要初始化一个空列表\n",
    "'''\n",
    "l_accesscount = [0] * tot_fqdn # 访问次数\n",
    "l_multiaccesscount = [0] * tot_fqdn # 多次访问次数\n",
    "# l_multiaccessip = [set() for _ in range(tot_fqdn)] # 多次访问IP\n",
    "l_accesstime = [[0] * 24 for _ in range(tot_fqdn)] # 按小时统计访问次数\n",
    "l_accessdate = [[0] * 92 for _ in range(tot_fqdn)] # 按日期统计访问次数\n",
    "l_accessip = [set() for _ in range(tot_fqdn)] # 访问IP\n",
    "l_ipcountry = [set() for _ in range(tot_fqdn)] # 访问IP所在国家\n",
    "l_ipcity = [set() for _ in range(tot_fqdn)] # 访问IP所在城市\n",
    "l_ipisp = [set() for _ in range(tot_fqdn)] # 访问IP ISP\n",
    "\n",
    "date2 = date(2020, 3, 1) # 开始日期\n",
    "\n",
    "# count\n",
    "\n",
    "for access in l_access:\n",
    "    num = int(access[0][5:]) # 域名编号\n",
    "    access_ip = access[1] # 访问IP\n",
    "    access_count = int(access[2]) # 访问次数\n",
    "    access_time = access[3] # 访问时间\n",
    "\n",
    "    l_accesscount[num] += access_count # 编号为 num 的域名的访问次数\n",
    "    if access_count > 1:\n",
    "        l_multiaccesscount[num] += access_count  # 编号为 num 的域名的连续访问次数\n",
    "        # l_multiaccessip[num].add(access_ip) # 编号为 num 的域名的连续访问 IP\n",
    "    \n",
    "    l_accesstime[num][int(access_time[8:10])] += access_count # 编号为 num 的域名按小时统计时间段的访问次数\n",
    "\n",
    "    date1 = datetime.datetime.strptime(access_time[:8], '%Y%m%d').date()\n",
    "    l_accessdate[num][(date1 - date2).days] += access_count  # 编号为 num 的域名按日期统计访问次数\n",
    "\n",
    "    match_ip = l_ip[access_ip]\n",
    "    l_accessip[num].add(access_ip)  # 编号为 num 的域名的访问 IP 列表\n",
    "    l_ipcountry[num].add(match_ip[0])  # 编号为 num 的域名的访问 IP 地址的国家列表\n",
    "    l_ipcity[num].add(match_ip[2])  # 编号为 num 的域名的访问 IP 地址的城市列表\n",
    "    l_ipisp[num].add(match_ip[5])  # 编号为 num 的域名的访问 IP 地址的 ISP 列表\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 flint\n",
    "\n",
    "选取的特征有\n",
    "\n",
    "- 解析次数\n",
    "- 按日期统计解析次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_notip = [] # 解析结果不是 IP 的域名\n",
    "\n",
    "tot_fqdn = 20512 # fqdn 编号总数\n",
    "l_flintcount = [0] * tot_fqdn # 解析次数\n",
    "l_flintdate = [[0] * 92 for _ in range(tot_fqdn)] # 按日期统计解析次数\n",
    "\n",
    "for flint in l_flint:\n",
    "    num = int(flint[0][5:])  # 域名编号\n",
    "    typenum = int(flint[1])  # 域名类型\n",
    "    if flint[2][0:4] == 'fqdn':\n",
    "        refernum = int(flint[2][5:]) # 域名引用编号\n",
    "        count = int(flint[3]) # 域名引用访问次数\n",
    "        date = datetime.datetime.strptime(flint[4], '%Y%m%d').date() # 域名引用访问时间\n",
    "        l_notip.append([num, typenum, refernum, count, date])\n",
    "    else:\n",
    "        ip = flint[2]  # 域名 IP\n",
    "        count = int(flint[3])  # 域名访问次数\n",
    "        date = datetime.datetime.strptime(flint[4], '%Y%m%d').date()  # 域名访问时间\n",
    "        \n",
    "    \n",
    "    l_flintcount[num] += count # 编号为 num 的域名的解析次数\n",
    "    l_flintdate[num][(date - date2).days] += count # 编号为 num 的域名按日期统计解析次数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 whois\n",
    "\n",
    "选取的特征有\n",
    "\n",
    "- 域名创建日期\n",
    "- 域名过期日期\n",
    "- 域名更新次数\n",
    "- 域名 DNS 服务器数\n",
    "- 域名管理员邮箱数\n",
    "- 域名注册国家数\n",
    "- 域名注册邮箱数\n",
    "- 域名注册省份数\n",
    "- 域名注册邮箱数\n",
    "- 域名 DNS 服务器列表数\n",
    "- 域名 DNS 服务器数\n",
    "- 域名注册商数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_whoiscreatedate = [10**6] * tot_fqdn # 域名创建日期\n",
    "l_whoisexpiredate = [0] * tot_fqdn # 域名过期日期\n",
    "l_whoisupdatedate = [set() for _ in range(tot_fqdn)]  # 域名更新日期\n",
    "\n",
    "l_whoisnameserver = [set() for _ in range(tot_fqdn)]\n",
    "l_whoisadminemail = [set() for _ in range(tot_fqdn)]\n",
    "l_whoisregistercountry = [set() for _ in range(tot_fqdn)]\n",
    "l_whoisregisteremail = [set() for _ in range(tot_fqdn)]\n",
    "l_whoisregisterstate = [set() for _ in range(tot_fqdn)]\n",
    "l_whoistechemial = [set() for _ in range(tot_fqdn)]\n",
    "l_whoisserverlist = [set() for _ in range(tot_fqdn)]\n",
    "l_whoisserver = [set() for _ in range(tot_fqdn)]\n",
    "l_whoissponsor = [set() for _ in range(tot_fqdn)]\n",
    "\n",
    "for whois in l_whois:\n",
    "    num = int(whois['fqdn_no'][5:]) # 域名编号\n",
    "    if whois['createddate'] != None:\n",
    "        l_whoiscreatedate[num] = min(int(whois['createddate']/86400000), l_whoiscreatedate[num])  # 域名创建日期\n",
    "    if whois['expiresdate'] != None:\n",
    "        l_whoisexpiredate[num] = max(int(whois['expiresdate']/86400000), l_whoisexpiredate[num])  # 域名过期日期\n",
    "    if whois['updateddate'] != None:\n",
    "        l_whoisupdatedate[num].add(int(whois['updateddate']))  # 域名更新日期\n",
    "    if whois['nameservers'] != None:\n",
    "        l_whoisnameserver[num].update(whois['nameservers'])\n",
    "    if whois['admin_email'] != None:\n",
    "        l_whoisadminemail[num].update(whois['admin_email'])  # 域名的管理员邮箱\n",
    "    if whois['registrant_country'] != None:\n",
    "        l_whoisregistercountry[num].update(whois['registrant_country'])  # 域名的注册国家\n",
    "    if whois['registrant_email'] != None:\n",
    "        l_whoisregisteremail[num].update(whois['registrant_email'])  # 域名的注册邮箱\n",
    "    if whois['registrant_state'] != None:\n",
    "        l_whoisregisterstate[num].update(whois['registrant_state'])  # 域名的注册省份\n",
    "    if whois['tech_email'] != None:\n",
    "        l_whoistechemial[num].update(whois['tech_email'])  # 域名的注册邮箱\n",
    "    if whois['r_whoisserver_list'] != None:\n",
    "        l_whoisserverlist[num].update(whois['r_whoisserver_list'])  # 域名的 DNS 服务器列表\n",
    "    if whois['whoisserver'] != None:\n",
    "        l_whoisserver[num].update(whois['whoisserver'])  # 域名的 DNS 服务器\n",
    "    if whois['sponsoring'] != None:\n",
    "        l_whoissponsor[num].update(whois['sponsoring'])  # 域名的注册商\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 label\n",
    "\n",
    "提取标签，并为拆分训练集、测试集做准备。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label_all = [-1] * tot_fqdn # 域名标签\n",
    "\n",
    "for label in l_label:\n",
    "    num = int(label[0][5:])  # 域名编号\n",
    "    Label_all[num] = int(label[1])  # 域名标签"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 fqdn\n",
    "\n",
    "进一步处理 2.1 - 2.4 得到的内容。\n",
    "\n",
    "提取 fqdn 中的特征：\n",
    "\n",
    "- 字符个数\n",
    "- 数字个数\n",
    "- 普通字符个数\n",
    "- 特殊字符个数\n",
    "- 单词字母个数\n",
    "- 深度\n",
    "\n",
    "并将所有内容整合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature_all = []  # 合并信息\n",
    "\n",
    "for fqdn in l_fqdn:\n",
    "    url = fqdn[0] # 域名\n",
    "    num = int(fqdn[1][5:]) # 域名编号\n",
    "\n",
    "    n_character = 0 # 字符个数\n",
    "    n_digit = 0 # 数字个数\n",
    "    n_normal = 0 # 普通字符个数\n",
    "    n_special = 0 # 特殊字符个数\n",
    "    n_alpha = 0 # 单词字母个数\n",
    "    n_depth = 0 # 深度\n",
    "    \n",
    "    n_access = l_accesscount[num] # 访问次数\n",
    "    n_multiaccess = l_multiaccesscount[num] # 连续访问次数\n",
    "    n_access_ip = len(l_accessip[num]) # 访问 IP 数\n",
    "    n_access_country = len(l_ipcountry[num]) # 访问国家数\n",
    "    n_access_city = len(l_ipcity[num]) # 访问城市数\n",
    "    n_access_isp = len(l_ipisp[num]) # 访问 ISP 数\n",
    "    n_access_byhour = l_accesstime[num]  # 按小时统计访问次数\n",
    "    n_access_bydate = l_accessdate[num]  # 按日期统计访问次数\n",
    "\n",
    "    n_flint = l_flintcount[num] # 解析次数\n",
    "    n_flint_bydate = l_flintdate[num] # 按日期统计解析次数\n",
    "\n",
    "    n_whois_create = l_whoiscreatedate[num] # 域名创建日期\n",
    "    n_whois_expire = l_whoisexpiredate[num] # 域名过期日期\n",
    "    n_whois_update = len(l_whoisupdatedate[num]) # 域名更新次数\n",
    "    n_whois_nameserver = len(l_whoisnameserver[num]) # 域名 DNS 服务器数\n",
    "    n_whois_adminemail = len(l_whoisadminemail[num]) # 域名管理员邮箱数\n",
    "    n_whois_registercountry = len(l_whoisregistercountry[num]) # 域名注册国家数\n",
    "    n_whois_registeremail = len(l_whoisregisteremail[num]) # 域名注册邮箱数\n",
    "    n_whois_registerstate = len(l_whoisregisterstate[num]) # 域名注册省份数\n",
    "    n_whois_techemial = len(l_whoistechemial[num]) # 域名注册邮箱数\n",
    "    n_whois_serverlist = len(l_whoisserverlist[num]) # 域名 DNS 服务器列表数\n",
    "    n_whois_server = len(l_whoisserver[num]) # 域名 DNS 服务器数\n",
    "    n_whois_sponsor = len(l_whoissponsor[num]) # 域名注册商数\n",
    "\n",
    "    is_normal_suffix = 0 # 是否是普通后缀\n",
    "    commonRootList = ['cn', 'com', 'net', 'org', 'gov', 'info', 'edu']\n",
    "    if url.split('.')[-1] in commonRootList:\n",
    "        is_normal_suffix = 1\n",
    "\n",
    "    is_alpha = False # 是否为单词字母\n",
    "\n",
    "    for i in range(len(url)):\n",
    "        if url[i] == '[':\n",
    "            is_alpha = True\n",
    "        elif url[i] == ']':\n",
    "            is_alpha = False\n",
    "        else:\n",
    "            n_character += 1\n",
    "            \n",
    "            if url[i].isalnum():\n",
    "                n_normal += 1\n",
    "            else:\n",
    "                n_special += 1\n",
    "\n",
    "            if url[i].isdigit():\n",
    "                n_digit += 1\n",
    "\n",
    "            if is_alpha:\n",
    "                n_alpha += 1\n",
    "                \n",
    "            if url[i] == '.':\n",
    "                n_depth += 1\n",
    "    \n",
    "    tmp = [n_character, n_digit, n_normal, n_special, n_alpha, n_depth, is_normal_suffix,\n",
    "           n_access, n_multiaccess, n_access_ip, n_access_country, n_access_city, n_access_isp,\n",
    "           n_flint,\n",
    "           n_whois_create, n_whois_expire, n_whois_update, n_whois_nameserver, n_whois_adminemail, n_whois_registercountry, n_whois_registeremail, n_whois_registerstate, n_whois_techemial, n_whois_serverlist, n_whois_server, n_whois_sponsor]\n",
    "    tmp.extend(n_access_byhour) # 拆分列表\n",
    "    tmp.extend(n_access_bydate)\n",
    "    tmp.extend(n_flint_bydate)\n",
    "    Feature_all.append(tmp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 训练与预测\n",
    "\n",
    "首先，把没有给 label 的都当作非恶意域名。\n",
    "\n",
    "然后选取所有的恶意域名、及约1500个非恶意域名做训练集；所有没有label的做测试集。\n",
    "\n",
    "区分是否为恶意域名。\n",
    "\n",
    "工具直接使用 xgboost。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_num_binary = []  # 恶意域名测试集编号\n",
    "test_feature_binary = []  # 恶意域名测试集特征\n",
    "test_label_binary = []  # 恶意域名测试集标签\n",
    "\n",
    "train_random_binary = [] # 随机域名训练集编号\n",
    "\n",
    "for i in range(tot_fqdn):\n",
    "    if Label_all[i] == -1:\n",
    "        '''\n",
    "        train_random_binary.append([i, Feature_all[i], 0])\n",
    "        '''\n",
    "        test_num_binary.append(i)\n",
    "        test_feature_binary.append(Feature_all[i])\n",
    "\n",
    "test_label_tmp = [0] * len(test_num_binary)\n",
    "\n",
    "tot_round = 500\n",
    "\n",
    "for itera in range(tot_round):\n",
    "    train_feature_binary = []  # 恶意域名训练集特征\n",
    "    train_label_binary = []  # 恶意域名训练集标签\n",
    "    train_random_binary = random.sample(test_feature_binary, 1600)\n",
    "    for i in range(tot_fqdn):\n",
    "        if Label_all[i] != -1:\n",
    "            train_feature_binary.append(Feature_all[i])\n",
    "            train_label_binary.append(1)\n",
    "    for i in range(len(train_random_binary)):\n",
    "        train_feature_binary.append(train_random_binary[i])\n",
    "        train_label_binary.append(0)\n",
    "\n",
    "    params = { # 玄学调参\n",
    "        'booster': 'gbtree',\n",
    "        'objective': 'binary:logistic',\n",
    "        'gamma': 0.1,\n",
    "        'max_depth': 6,\n",
    "        'lambda': 2,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'min_child_weight': 1,\n",
    "        'eta': 0.05,\n",
    "        'eval_metric': 'logloss'\n",
    "    }\n",
    "\n",
    "    # 构造训练集\n",
    "    dtrain = xgb.DMatrix(train_feature_binary, train_label_binary)\n",
    "    num_rounds = 500\n",
    "    # xgboost模型训练\n",
    "    model = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "    # 对测试集进行预测\n",
    "    dtest = xgb.DMatrix(test_feature_binary)\n",
    "    test_label_binary_float = model.predict(dtest)\n",
    "\n",
    "    for i in range(len(test_label_binary_float)):\n",
    "        test_label_tmp[i] += round(test_label_binary_float[i])\n",
    "    \n",
    "    print(itera, end='...')\n",
    "        \n",
    "for i in range(len(test_label_tmp)):\n",
    "    test_label_tmp[i] = test_label_tmp[i] / tot_round\n",
    "\n",
    "test_num = []\n",
    "test_feature = []\n",
    "\n",
    "# 将恶意域名单独列出来再细分\n",
    "test_label_binary = [0] * len(test_label_tmp)\n",
    "for i in range(len(test_label_tmp)):\n",
    "    test_label_binary[i] = round(test_label_tmp[i])\n",
    "    if test_label_binary[i] == 1:\n",
    "        test_num.append(test_num_binary[i])\n",
    "        test_feature.append(test_feature_binary[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"guilty.csv\", \"w\") as csvfile:\n",
    "    csvfile.write(\"fqdn_no,prob\\n\")\n",
    "    for i in range(len(test_label_tmp)):\n",
    "        csvfile.write(\n",
    "            \"fqdn_\" + str(test_num_binary[i]) + \",\" + str(test_label_tmp[i]) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看看找到了几个恶意域名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "542\n"
     ]
    }
   ],
   "source": [
    "print(len(test_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "区分恶意域名种类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = []\n",
    "train_feature = []\n",
    "train_label = []\n",
    "\n",
    "test_label = []\n",
    "\n",
    "for i in range(len(l_fqdn)):\n",
    "    if Label_all[i] != -1:\n",
    "        train_num.append(i)\n",
    "        train_feature.append(Feature_all[i])\n",
    "        train_label.append(Label_all[i])\n",
    "\n",
    "params = {  # 玄学调参\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 9,\n",
    "    'gamma': 0.0566,\n",
    "    'max_depth': 4,\n",
    "    'lambda': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 2.6,\n",
    "    'eta': 0.092\n",
    "}\n",
    "\n",
    "# 构造训练集\n",
    "dtrain = xgb.DMatrix(train_feature, train_label)\n",
    "num_rounds = 650\n",
    "# xgboost模型训练\n",
    "model = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "# 对测试集进行预测\n",
    "dtest = xgb.DMatrix(test_feature)\n",
    "test_label = model.predict(dtest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出结果\n",
    "with open(\"result.csv\", \"w\") as csvfile:\n",
    "    csvfile.write(\"fqdn_no,family_no\\n\")\n",
    "    for row in l_label:\n",
    "        csvfile.write(row[0] + \",\" + row[1] + \"\\n\")\n",
    "    for i in range(len(test_num)):\n",
    "        csvfile.write(\"fqdn_\" + str(test_num[i]) + \",\" + str(test_label[i]) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集总数：\n",
      "476\n",
      "结果总数：\n",
      "542\n",
      "训练集类别数量：\n",
      "[336, 6, 18, 17, 24, 8, 57, 6, 4]\n",
      "期望结果数量：\n",
      "[383, 7, 20, 19, 27, 9, 65, 7, 5]\n",
      "结果类别数量：\n",
      "[391, 7, 15, 14, 14, 12, 83, 4, 2]\n",
      "与期望差异值：\n",
      "[8, 0, 5, 5, 13, 3, 18, 3, 3]\n",
      "训练集类别占比：\n",
      "[0.706, 0.013, 0.038, 0.036, 0.05, 0.017, 0.12, 0.013, 0.008]\n",
      "结果类别占比：\n",
      "[0.721, 0.013, 0.028, 0.026, 0.026, 0.022, 0.153, 0.007, 0.004]\n"
     ]
    }
   ],
   "source": [
    "train_count = [0] * 9\n",
    "train_percent = [0] * 9\n",
    "for key in train_label:\n",
    "  train_count[int(key)] += 1\n",
    "for i in range(9):\n",
    "  train_percent[i] = round(train_count[i] / len(train_label), 3)\n",
    "\n",
    "\n",
    "result_count = [0] * 9\n",
    "result_percent = [0] * 9\n",
    "for key in test_label:\n",
    "  result_count[int(key)] += 1\n",
    "for i in range(9):\n",
    "  result_percent[i] = round(result_count[i] / len(test_label), 3)\n",
    "\n",
    "expected_count = [0] * 9\n",
    "diff_count = [0] * 9\n",
    "for i in range(9):\n",
    "  expected_count[i] = round((train_count[i] / len(train_label)) * len(test_label))\n",
    "  diff_count[i] = abs(result_count[i] - expected_count[i])\n",
    "\n",
    "print(\"训练集总数：\")\n",
    "print(len(train_label))\n",
    "print(\"结果总数：\")\n",
    "print(len(test_label))\n",
    "\n",
    "print(\"训练集类别数量：\")\n",
    "print(train_count)\n",
    "print(\"期望结果数量：\")\n",
    "print(expected_count)\n",
    "print(\"结果类别数量：\")\n",
    "print(result_count)\n",
    "print(\"与期望差异值：\")\n",
    "print(diff_count)\n",
    "\n",
    "print(\"训练集类别占比：\")\n",
    "print(train_percent)\n",
    "print(\"结果类别占比：\")\n",
    "print(result_percent)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
